---
title: "ACML 2025 Tutorial on Towards Robust and Trustworthy Large Language Models: Issues and Mitigation Strategies"
layout: page
permalink: /
---

## Overview

Recent advancements in large language models (LLMs) have achieved superhuman performance across various domains and tasks, unlocking their potential in real-world applications.
However, robustness and trustworthiness issues hinder their reliable deployment.
Robustness issues refer to inconsistent model behaviors under equivalent conditions, such as sensitivity to minor prompt variations.
Trustworthiness encompasses issues like hallucinations--situations where LLMs produce factually incorrect or input-conflicting outputs--and fairness issues, including biases toward certain races, genders, value systems, or languages.
Addressing these is crucial for building reliable applications with these powerful yet vulnerable models.
We will conclude the tutorial by outlining future research directions in this area.

# Speakers

<div class="col-md-4">
    <div class="profile height150">
        <div><a href="https://brian-ckwu.github.io/"><img class="avatar-img" width=150 src="images/ckwu.jpg"> </a></div>
        <div style="margin-bottom:40px"><center><b> Cheng-Kuang Wu</b> <br> Appier </center></div>
    </div>
</div>
<div class="col-md-4">
    <div class="profile height150">
        <div><a href="https://zrt.wtf/"><img class="avatar-img" width=150 src="images/zhiruitam.jpeg"></a></div>
        <div style="margin-bottom:40px"><center><b>Zhi Rui Tam</b><br> NTU</center></div>
    </div>
</div>

<div class="col-md-4">
    <div class="profile height150">
        <div><a href="https://khhuang.me/"><img class="avatar-img" width=150 src="images/kuanhaohuang.jpg"></a></div>
        <div style="margin-bottom:40px"><center><b>Kuan-Hao Huang</b><br> Texas A&M University</center></div>
    </div>
</div>

## Outline


- Introduction
- Attacking and Jailbreaking
- Machine Unlearning
- Hallucinations
- Prompt Robustness
- Position and Order Biases
- Fairness and Social Bias
- Robustness for Multimodal LLMs
- Robustness of Reasoning Models

## Slides

<iframe 
  src="https://docs.google.com/viewer?embedded=true&url=https://llms-robustness-2025.github.io/ACML-2025.pdf" 
  width="100%" 
  height="400px"
  style="border:none;">
</iframe>


