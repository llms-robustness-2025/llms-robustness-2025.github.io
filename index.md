---
title: ACML 2025 Towards Robust and Trustworthy Large Language
Models - Issues and Mitigation Strategies 
layout: page
permalink: /
---

## Overview

Recent advancements in large language models (LLMs) have achieved superhuman performance across various domains and tasks, unlocking their potential in real-world applications.
However, robustness and trustworthiness issues hinder their reliable deployment.
Robustness issues refer to inconsistent model behaviors under equivalent conditions, such as sensitivity to minor prompt variations.
Trustworthiness encompasses issues like hallucinations--situations where LLMs produce factually incorrect or input-conflicting outputs--and fairness issues, including biases toward certain races, genders, value systems, or languages.
Addressing these is crucial for building reliable applications with these powerful yet vulnerable models.

We will conclude the tutorial by outlining future research directions in this area.

# Speakers
<div class="col-md-3">
    <div class="profile height150">
        <div><a href="http://kwchang.net"><img class="avatar-img" width=150 src="https://avatars2.githubusercontent.com/kaiweichang?v=3&s=400"></a></div>
        <div style="margin-bottom:40px"><center><b>Kai-Wei Chang</b><br> UCLA</center></div>
    </div>
</div>

<div class="col-md-3">
    <div class="profile height150">
        <div><a href="https://hhexiy.github.io/"><img class="avatar-img" width=150 src="images/he.png"> </a></div>
        <div style="margin-bottom:40px"><center><b> He He</b> <br> NYU </center></div>
    </div>
</div>
<div class="col-md-3">
    <div class="profile height150">
        <div><a href="https://robinjia.github.io/"><img class="avatar-img" width=150 src="https://avatars2.githubusercontent.com/robinjia?v=3&s=400"></a></div>
        <div style="margin-bottom:40px"><center><b>Robin Jia</b><br> USC</center></div>
    </div>
</div>

<div class="col-md-3">
    <div class="profile height150">
        <div><a href="https://sameersingh.org/"><img class="avatar-img" width=150 src="images/sameer.png"></a></div>
        <div style="margin-bottom:40px"><center><b>Sameer Singh</b><br> UC Irvine</center></div>
    </div>
</div>

## Outline

- Motivation and Overview 
- Finding Lack of Robustness (Attacks) 
    1. Writing Challenging Examples
    1. Generating Adversarial Examples
    1. Adversarial Trigger and Text Generation 
    1. Training Time Attack
- Making Models Robust (Defenses)
    1. Robustness to Spurious Correlations
    1. Adversarial Training for Defense
    1. Certified Robustness in NLP
    1. Test time-defense: detecting adversarial examples
- Conclusion, Future Directions, and Discussion 


## Slides
<iframe src="https://docs.google.com/presentation/d/e/2PACX-1vQObhZjgRpHPVStVU2V87P-E4LgsD764B2bY4CUOhOEhORPMXQOnKpmxmtoePFvBW81NDrCn3VaOAT8/embed?start=false&loop=false&delayms=3000" frameborder="0" width="960" height="569" allowfullscreen="true" mozallowfullscreen="true" webkitallowfullscreen="true"></iframe>
