---
layout: page
title: Readings 
permalink: /readings
---

### Attacking and Jailbreaking
1. [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043). 
   *Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, Matt Fredrikson*.
   arXiv preprint 2023.
1. [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140). 
   *Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, Tong Sun*. 
   COLM 2024.
1. [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/2401.06373). 
   *Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi*. 
   ACL 2024.
1. [Jailbreaking Large Language Models with Symbolic Mathematics](https://arxiv.org/abs/2409.11445).
   *Emet Bethany, Mazal Bethany, Juan Arturo Nolazco Flores, Sumit Kumar Jha, Peyman Najafirad*.
   arXiv preprint 2024.
1. [Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks](https://arxiv.org/abs/2302.05733).
   *Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, Tatsunori Hashimoto*.
   arXiv preprint 2023.
1. [Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419).
   *Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong*.
   arXiv preprint 2023.
1. [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://arxiv.org/abs/2310.04451).
   *Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao*.
   ICLR 2024.
1. [Multilingual Jailbreak Challenges in Large Language Models](https://arxiv.org/abs/2310.06474).
   *Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing*.
   ICLR 2024.
1. [Jailbreak Attacks and Defenses Against Large Language Models: A Survey](https://arxiv.org/abs/2407.04295).
   *Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, Qi Li*.
   arXiv preprint 2024.
1. [Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM](https://arxiv.org/abs/2309.14348).
   *Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen*.
   arXiv preprint 2024.
1. [SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2310.03684).
   *Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas*.
   arXiv preprint 2023.
1. [Defending LLMs against Jailbreaking Attacks via Backtranslation](https://arxiv.org/abs/2402.16459).
   *Yihan Wang, Zhouxing Shi, Andrew Bai, Cho-Jui Hsieh*.
   ACL Findings 2024.

### Machine Unlearning
1. [Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238).
   *Ronen Eldan, Mark Russinovich*.
   arXiv preprint 2023.
1. [Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning](https://arxiv.org/abs/2404.05868).
   *Ruiqi Zhang, Licong Lin, Yu Bai, Song Mei*.
   COLM 2024.
1. [Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness](https://arxiv.org/abs/2506.05735).
   *Rongzhe Wei, Peizhi Niu, Hans Hao-Hsun Hsu, Ruihan Wu, Haoteng Yin, Mohsen Ghassemi, Yifan Li, Vamsi K. Potluru, Eli Chien, Kamalika Chaudhuri, Olgica Milenkovic, Pan Li*.
   NeurIPS 2025.
1. [Machine Unlearning: A Survey](https://arxiv.org/abs/2306.03558).
   *Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, Philip S. Yu*.
   arXiv preprint 2023.
1. [Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?](https://arxiv.org/abs/2505.19855).
   *Zexi Li, Xiangzhu Wang, William F. Shen, Meghdad Kurmanji, Xinchi Qiu, Dongqi Cai, Chao Wu, Nicholas D. Lane*.
   arXiv preprint 2025.
1. [Locating and Editing Factual Associations in GPT](https://arxiv.org/abs/2202.05262).
   *Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov*.
   NeurIPS 2022.
1. [Mass-Editing Memory in a Transformer](https://arxiv.org/abs/2210.07229).
   *Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David Bau*.
   ICLR 2023.
1. [PMET: Precise Model Editing in a Transformer](https://arxiv.org/abs/2308.08742).
   *Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, Jie Yu*.
   AAAI 2024.
1. [A Unified Framework for Model Editing](https://arxiv.org/abs/2403.14236).
   *Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli*.
   EMNLP-Findings 2024.
1. [AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models](https://arxiv.org/abs/2410.02355).
   *Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Shi Jie, Xiang Wang, Xiangnan He, Tat-seng Chua*.
   ICLR 2025.



### Hallucinations

### Prompt Robustness

### Position and Order Biases

### Robustness of Reasoning Models

1. [Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens](https://arxiv.org/abs/2508.01191).
   *Chengshuai Zhao, Zhen Tan, Pingchuan Ma, Dawei Li, Bohan Jiang, Yancheng Wang, Yingzhen Yang, Huan Liu*.
   arXiv preprint 2025.
1. [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941).
   *Parshin Shojaee, Iman Mirzadeh, Keivan Alizadeh, Maxwell Horton, Samy Bengio, Mehrdad Farajtabar*.
   arXiv preprint 2025.
1. [An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems](https://arxiv.org/abs/2508.08833).
   *Yuren Hao, Xiang Wan, Chengxiang Zhai*.
   arXiv preprint 2025.
1. [Can language models perform robust reasoning in chain-of-thought prompting with noisy rationales?](https://arxiv.org/pdf/2410.23856v1).
   *Zhou, Z., Tao, R., Zhu, J., Luo, Y., Wang, Z., Han, B.*.
   Advances in Neural Information Processing Systems 2024.
1. [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276).
   *David Guzman Piedrahita et al*.
   COLM 2025.
1. [MoReBench: Evaluating Procedural and Pluralistic Moral Reasoning in Language Models, More than Outcomes](https://arxiv.org/abs/2510.16380).
   *Yu Ying Chiu, Michael S. Lee, Rachel Calcott, Brandon Handoko, Paul de Font-Reaulx, Paula Rodriguez, Chen Bo Calvin Zhang, Ziwen Han, Udari Madhushani Sehwag, Yash Maurya, Christina Q. Knight, Harry R. Lloyd, Florence Bacus, Mantas Mazeika, Bing Liu, Yejin Choi, Mitchell L. Gordon, Sydney Levine*.
   arXiv preprint 2025.


### Fairness and Social Bias

1. [CulturalBench: A Robust, Diverse, and Challenging Cultural Benchmark](https://arxiv.org/abs/2410.02677).
   *Yu Ying Chiu et al.*
   Benchmark / arXiv preprint.
1. [Culture Cartography: Evaluating Global Cultural Coverage in Large Language Models](https://aclanthology.org/2025.emnlp-main.91.pdf).
   *Caleb Ziems et al.*
   arXiv preprint 2024.
1. [INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge](https://arxiv.org/abs/2411.19799).
   *Angelika Romanou, Negar Foroutan, Anna Sotnikova, Zeming Chen, Sree Harsha Nelaturu, Sumeet Singh, Rakesh Maheshwary, Marco Altomare, Mohamed A. Haggag, Anagha Snegha, â€¦, Antoine Bosselut*.
   arXiv preprint 2024.
1. [Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures](https://arxiv.org/abs/2510.24081).
   *Global PIQA Collaboration*.
   arXiv preprint 2025.
1. [DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life](https://arxiv.org/abs/2410.02683).
   *Yu Ying Chiu, Liwei Jiang, Yejin Choi*.
   ICLR 2025.
1. [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values under Moral Dilemmas](https://arxiv.org/abs/2505.14633).
   *Yu Ying Chiu, Zhilin Wang, Sharan Maiya, Yejin Choi, Kyle Fish, Sydney Levine, Evan Hubinger*.
   ACL Findings 2025.
1. [Modular Pluralism: Pluralistic Alignment via Multi-LLM Collaboration](https://arxiv.org/abs/2406.15951).
    *Shangbin Feng, Taylor Sorensen, Yuhan Liu, Jillian Fisher, Chan Young Park, Yejin Choi, Yulia Tsvetkov*.
    EMNLP 2024.
1. [Persona Vectors: Monitoring and Controlling Character Traits in Language Models](https://arxiv.org/abs/2507.21509).
    *Runjin Chen, Andy Arditi, Henry Sleight, Owain Evans, Jack Lindsey*.
    arXiv preprint 2025.
1. [Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability](https://arxiv.org/abs/2510.06084).
    *Taylor Sorensen, Benjamin Newman, Jared Moore, Chan Young Park, Jillian Fisher, Niloofar Mireshghallah, Liwei Jiang, Yejin Choi*.
    arXiv preprint 2025.

### Robustness for Multimodal LLMs

1. [MedVoiceBias: A Controlled Study of Audio LLM Behavior in Clinical Decision-Making](https://arxiv.org/abs/2511.06592).
   *Zhi Rui Tam, Yun-Nung Chen*.
   arXiv preprint 2025.
1. [Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models](https://arxiv.org/abs/2104.08666).
   *Tejas Srinivasan, Yonatan Bisk*.
   GeBNLP (NAACL Workshop) 2022.
1. [Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective](https://arxiv.org/abs/2407.02814).
   *Zhaotian Weng, Zijun Gao, Jerone Andrews, Jieyu Zhao*.
   EMNLP 2024.
1. [The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks](https://arxiv.org/abs/2509.18234).
   *Yu Gu, Jingjing Fu, Xiaodong Liu, Jeya Maria Jose Valanarasu, Noel Codella, Reuben Tan, Qianchu Liu, Ying Jin, Sheng Zhang, Jinyu Wang, Rui Wang, Lei Song, Guanghui Qin, Naoto Usuyama, Cliff Wong, Cheng Hao, Hohin Lee, Praneeth Sanapathi, Sarah Hilado, Bian Jiang, Javier Alvarez-Valle, Mu Wei, Jianfeng Gao, Eric Horvitz, Matt Lungren, Hoifung Poon, Paul Vozila*.
   arXiv preprint 2025.
1. [What's Missing in Vision-Language Models? Probing Their Struggles with Causal Order Reasoning](https://arxiv.org/abs/2506.00869).
   *Zhaotian Weng, Haoxuan Li, Kuan-Hao Huang, Jieyu Zhao*.
   arXiv preprint 2025.
1. [Can Large Vision-Language Models Correct Semantic Grounding Errors By Themselves?](https://arxiv.org/abs/2404.06510).
   *Yuan-Hong Liao, Rafid Mahmood, Sanja Fidler, David Acuna*.
   CVPR 2025.
1. [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966).
   *Alex Su, Haozhe Wang, Weiming Ren, Fangzhen Lin, Wenhu Chen*.
   arXiv preprint 2025.
1. [GRIT: Teaching MLLMs to Think with Images](https://arxiv.org/abs/2505.15879).
   *Yue Fan, Xuehai He, Diji Yang, Kaizhi Zheng, Ching-Chen Kuo, Yuting Zheng, Sravana Jyothi Narayanaraju, Xinze Guan, Xin Eric Wang*.
   arXiv preprint 2025.
1. [More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models](https://arxiv.org/abs/2505.21523).
   *Authors as listed in the paper*.
   arXiv preprint.