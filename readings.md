---
layout: page
title: Readings 
permalink: /readings
---

### Attacking and Jailbreaking
1. [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/abs/2307.15043). 
   *Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, Matt Fredrikson*.
   arXiv preprint 2023.
1. [AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models](https://arxiv.org/abs/2310.15140). 
   *Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, Tong Sun*. 
   COLM 2024.
1. [How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs](https://arxiv.org/abs/2401.06373). 
   *Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi*. 
   ACL 2024.
1. [Jailbreaking Large Language Models with Symbolic Mathematics](https://arxiv.org/abs/2409.11445).
   *Emet Bethany, Mazal Bethany, Juan Arturo Nolazco Flores, Sumit Kumar Jha, Peyman Najafirad*.
   arXiv preprint 2024.
1. [Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks](https://arxiv.org/abs/2302.05733).
   *Daniel Kang, Xuechen Li, Ion Stoica, Carlos Guestrin, Matei Zaharia, Tatsunori Hashimoto*.
   arXiv preprint 2023.
1. [Jailbreaking Black Box Large Language Models in Twenty Queries](https://arxiv.org/abs/2310.08419).
   *Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George J. Pappas, Eric Wong*.
   arXiv preprint 2023.
1. [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://arxiv.org/abs/2310.04451).
   *Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao*.
   ICLR 2024.
1. [Multilingual Jailbreak Challenges in Large Language Models](https://arxiv.org/abs/2310.06474).
   *Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing*.
   ICLR 2024.
1. [Jailbreak Attacks and Defenses Against Large Language Models: A Survey](https://arxiv.org/abs/2407.04295).
   *Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, Qi Li*.
   arXiv preprint 2024.
1. [Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM](https://arxiv.org/abs/2309.14348).
   *Bochuan Cao, Yuanpu Cao, Lu Lin, Jinghui Chen*.
   arXiv preprint 2024.
1. [SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks](https://arxiv.org/abs/2310.03684).
   *Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas*.
   arXiv preprint 2023.
1. [Defending LLMs against Jailbreaking Attacks via Backtranslation](https://arxiv.org/abs/2402.16459).
   *Yihan Wang, Zhouxing Shi, Andrew Bai, Cho-Jui Hsieh*.
   ACL Findings 2024.

### Machine Unlearning
1. [Who's Harry Potter? Approximate Unlearning in LLMs](https://arxiv.org/abs/2310.02238).
   *Ronen Eldan, Mark Russinovich*.
   arXiv preprint 2023.
1. [Negative Preference Optimization: From Catastrophic Collapse to Effective Unlearning](https://arxiv.org/abs/2404.05868).
   *Ruiqi Zhang, Licong Lin, Yu Bai, Song Mei*.
   COLM 2024.
1. [Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness](https://arxiv.org/abs/2506.05735).
   *Rongzhe Wei, Peizhi Niu, Hans Hao-Hsun Hsu, Ruihan Wu, Haoteng Yin, Mohsen Ghassemi, Yifan Li, Vamsi K. Potluru, Eli Chien, Kamalika Chaudhuri, Olgica Milenkovic, Pan Li*.
   NeurIPS 2025.
1. [Machine Unlearning: A Survey](https://arxiv.org/abs/2306.03558).
   *Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, Philip S. Yu*.
   arXiv preprint 2023.
1. [Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?](https://arxiv.org/abs/2505.19855).
   *Zexi Li, Xiangzhu Wang, William F. Shen, Meghdad Kurmanji, Xinchi Qiu, Dongqi Cai, Chao Wu, Nicholas D. Lane*.
   arXiv preprint 2025.
1. [Locating and Editing Factual Associations in GPT](https://arxiv.org/abs/2202.05262).
   *Kevin Meng, David Bau, Alex Andonian, Yonatan Belinkov*.
   NeurIPS 2022.
1. [Mass-Editing Memory in a Transformer](https://arxiv.org/abs/2210.07229).
   *Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, David Bau*.
   ICLR 2023.
1. [PMET: Precise Model Editing in a Transformer](https://arxiv.org/abs/2308.08742).
   *Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, Jie Yu*.
   AAAI 2024.
1. [A Unified Framework for Model Editing](https://arxiv.org/abs/2403.14236).
   *Akshat Gupta, Dev Sajnani, Gopala Anumanchipalli*.
   EMNLP-Findings 2024.
1. [AlphaEdit: Null-Space Constrained Knowledge Editing for Language Models](https://arxiv.org/abs/2410.02355).
   *Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Shi Jie, Xiang Wang, Xiangnan He, Tat-seng Chua*.
   ICLR 2025.



### Hallucinations

### Prompt Robustness

### Position and Order Biases

### Fairness and Social Bias

### Robustness for Multimodal LLMs

### Robustness of Reasoning Models


